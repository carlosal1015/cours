\documentclass{yann}

\newcommand\PSdot{\PS⋅⋅}
\newcommand\SpE{\mathscr{S}^+(E)}
\newcommand\SppE{\mathscr{S}^{++}(E)}
\newcommand\SnRp{\mathrm{S}^+_n(ℝ)}
\newcommand\SnRpp{\mathrm{S}^{++}_n(ℝ)}
\newcommand\Pmixte[3]{\cro{#1,#2,#3}}
\newcommand\Signe{\mathop{\mathrm{signe}}}

\begin{document}
\title{Algèbre bilinéaire}
\maketitle

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
\section{Espaces préhilbertiens réels}

\Para{Définition}
Soit $E$ un $ℝ$-espace vectoriel de dimension finie ou infinie.
Un \emph{produit scalaire} sur $E$ est une forme bilinéaire symétrique définie positive sur $E$.
Autrement dit, un \emph{produit scalaire} sur $E$ est une application $\Fn{φ}{E^2}{ℝ}$
\begin{enumerate}
\item \emph{bilinéaire}:
  $∀(x,y,z)∈E^3$, $∀(λ,μ)∈ℝ^2$,
  \[ φ(λx+μy,z) = λφ(x,z) + μφ(y,z), \]
  \[ φ(x,λy+μz) = λφ(x,y) + μφ(x,z). \]
\item \emph{symétrique}: $∀(x,y)∈E^2$,
  \[ φ(y,x) = φ(x,y). \]
\item \emph{définie positive}: $∀x∈E∖\acco{0_E}$,
  \[ φ(x,x) > 0. \]
\end{enumerate}

\Para{Proposition}
En pratique, on vérifie généralement:
\begin{enumerate}
\item $φ\colon E^2 \toℝ$ est bien définie;
\item pour tout $(x,y,z)∈E^3$, pour tout $(λ,μ)∈ℝ^2$,
  \begin{enumerate}
  \item $φ(x,y) = φ(y,x)$;
  \item $φ(x,λy+μz) = λφ(x,y) + μφ(x,z)$;
  \item $φ(x,x)≥0$;
  \item $φ(x,x) = 0 \; \Longrightarrow \; x = 0_E$.
  \end{enumerate}
\end{enumerate}

\Para{Définition}
Un \emph{espace préhilbertien réel} $(E,φ)$ est un $ℝ$-espace vectoriel $E$ muni d'un produit scalaire $φ$ sur $E$.
À la place de $φ(x,y)$, on note généralement $\langle x, y \rangle$, $\langle x \mid y \rangle$, $(x,y)$, $(x\mid y)$ ou encore $x⋅y$.

\Para{Théorème}[inégalité de Cauchy-Schwarz]
Soit $(E,\PSdot)$ un espace préhilbertien réel.
Alors pour tout $(x,y)∈E^2$,
\[ \BigAbs{ \PS xy } ≤ √{\PS xx} √{\PS yy}. \]
De plus, on a égalité si et seulement si $x$ et $y$ sont liés.

\Para{Proposition}
Soit $(E,\PSdot)$ un espace préhilbertien réel.
On définit $\Fn NEℝ$ par $N(x) = √{\PS xx}$.
Alors $N$ est une norme sur $E$.

\Para{Définition}
Soit $(E,\PSdot)$ un espace préhilbertien réel.
On appelle \emph{norme associée} au produit scalaire $\PSdot$ la norme définie par
\[ \Norm{x} = √{\PS xx} \]
Cela fournit une structure d'espace vectoriel normé sur $E$;
on peut donc parler de convergence sur $E$, de topologie sur $E$, etc.

\Para{Proposition}
Soit $(E,\PSdot)$ un espace préhilbertien réel et $(x,y)∈E^2$.
Alors on a
\begin{enumerate}
\item $\Norm{x+y}^2 = \Norm{x}^2 + \Norm{y}^2 + 2\PS xy$;
\item l'\emph{inégalité de Cauchy-Schwarz}:
  $\bigl| \PS xy \bigr| ≤\Norm x⋅\Norm y$;
\item l'\emph{identité du parallélogramme}:
  $\Norm{x+y}^2 + \Norm{x-y}^2 = 2\bigl( \Norm{x}^2 + \Norm{y}^2 \bigr)$;
\item l'\emph{identité de polarisation}:
  $\PS xy = \frac14 \bigl( \Norm{x+y}^2 - \Norm{x-y}^2 \bigr)$.
\end{enumerate}

\Para{Exemples}
\begin{enumerate}
\item Sur $E = ℝ^n$.
  Si $x = \nUplet x1n∈E$ et $y = \nUplet y1n∈E$,
  on pose \[ \PS xy = ∑_{k=1}^n x_k \, y_k. \]
\item Sur $E = \mathcal{C}([0,1],ℝ)$.
  Si $(f,g)∈E^2$,
  on pose \[ \PS fg = ∫_0^1 f(t) g(t) \D t. \]
\item Et bien d'autres, cf. exercices~\ref{exo:ps1}, \ref{exo:ps2}, \ref{exo:ps3}, \ref{exo:ps4}, \ref{exo:ps5} et~\ref{exo:ps6}.
\end{enumerate}

\Para{Notation}
À partir de maintenant, $E$ désignera systématiquement un espace préhilbertien réel, et son produit scalaire sera noté $\PSdot$.

% -----------------------------------------------------------------------------
\subsection{Vecteurs orthogonaux}

\Para{Définition}
Soit $(x,y)∈E^2$.
On dit que les vecteurs $x$ et $y$ sont \emph{orthogonaux} si et seulement si $\PS xy = 0$. On note $x \orth y$.

\Para{Définitions}
Soit $\mathcal{F} = \nUplet x1n$ une famille de vecteurs de $E$.
\begin{itemize}
\item La famille $\mathcal{F}$ est \emph{orthogonale}
  si et seulement si pour tout $(i,j)∈\ccro{1,n}^2$ tels que $i≠j$,
  on a $\PS{x_i}{x_j} = 0$.
\item La famille $\mathcal{F}$ est \emph{orthonormale} (ou \emph{orthonormée})
  si et seulement si elle est orthogonale et pour tout $i∈\ccro{1,n}$,
  on a $\Norm{x_i} = 1$.
\item La famille $\mathcal{F}$ est une \emph{base orthonormale}
  (ou \emph{base orthonormale})
  si et seulement si c'est une base et une famille orthonormale.
\end{itemize}

\Para{Proposition}
\begin{enumerate}
\item Une famille orthogonale dont tous les éléments sont non nuls est libre.
\item Une famille orthonormale est libre.
\item La famille $\nUplet x1n$ est orthonormale si et seulement si
  $∀(i,j)∈\ccro{1,n}^2$, $\PS{x_i}{x_j} = δ_{i,j}$.
\item Une base orthonormale est une famille orthonormale qui est également génératrice.
\end{enumerate}

\Para{Théorème}[Pythagore]
Soit $\nUplet x1n$ une famille orthogonale de vecteurs de $E$.
Alors \[ \left\| ∑_{i=1}^n x_i \right\|^2 = ∑_{i=1}^n \Norm{x_i}^2. \]

\Para{Théorème}[réciproque de Pythagore]
Soit $(x,y)∈E^2$.
Si $\Norm{x+y}^2 = \Norm{x}^2 + \Norm{y}^2$,
alors $x$ et $y$ sont orthogonaux.

\Para{Théorème}[procédé de Gram-Schmidt]
Soit $\nUplet x1p$ une famille \emph{libre} de vecteurs de $E$.
Alors il existe une \emph{unique} famille $\nUplet e1p$ de vecteurs de $E$
telle que
\begin{itemize}
\item la famille $\nUplet e1p$ est orthonormée,
\item pour tout $n∈\ccro{1,p}$, \[ \Vect \nUplet x1n = \Vect \nUplet e1n, \]
\item pour tout $n∈\ccro{1,p}$, $\PS{x_n}{e_n} > 0$.
\end{itemize}
De plus, on peut la calculer avec les formules suivantes:
\begin{itemize}
\item $\widetilde{e_1} = x_1$;
\item pour $n∈\ccro{2,p}$, on note
  \[ \widetilde{e_n} = x_n - ∑_{k=1}^{n-1} \PS{e_k}{x_n} e_k
  = x_n -∑_{k=1}^{n-1} \frac{\PS{\widetilde{e_k}}{x_n}}{\Norm{\widetilde{e_k}}^2} \widetilde{e_k}; \]
\item pour $n∈\ccro{1,p}$, on a
  $\DS e_n = \frac1{\Norm{\widetilde{e_n}}} \, \widetilde{e_n}$.
\end{itemize}

\Para{Corollaire}[procédé de Gram-Schmidt infini]
Soit $(x_n)_{n∈ℕ}$ une famille libre de vecteurs de $E$.
Alors il existe une unique famille $(e_n)_{n∈ℕ}$
telle que
\begin{itemize}
\item la famille $(e_n)_{n∈ℕ}$ est orthonormée,
\item pour tout $n∈ℕ$, \[ \Vect \nUplet x0n = \Vect \nUplet e0n, \]
\item pour tout $n∈ℕ$, $\PS{x_n}{e_n} > 0$.
\end{itemize}
De plus, on peut la calculer avec les formules suivantes:
\begin{itemize}
\item $\widetilde{e_0} = x_0$;
\item pour $n∈\Ns$, on note
  \[ \widetilde{e_n} = x_n - ∑_{k=0}^{n-1} \PS{e_k}{x_n} e_k
  = x_n -∑_{k=0}^{n-1} \frac{\PS{\widetilde{e_k}}{x_n}}{\Norm{\widetilde{e_k}}^2} \widetilde{e_k}; \]
\item pour $n∈ℕ$, on a
  $\DS e_n = \frac1{\Norm{\widetilde{e_n}}} \, \widetilde{e_n}$.
\end{itemize}

\Para{Proposition}[Bessel-Parseval en dimension finie]
Si $\nUplet e1n$ une base orthonormale de $E$,
alors pour tout $x∈E$, on a
\[ x = ∑_{k=1}^n \PS{e_k}{x} e_k \quad\text{et}\quad \Norm{x}^2 = ∑_{k=1}^n \PS{e_k}{x}^2. \]

\Para{Théorème}[inégalité de Bessel]
Soit $\mathcal{F} = (e_n)_{n∈ℕ}$ une famille orthonormée et $x∈E$.
Alors la série de terme général $\PS{e_n}{x}^2$ converge et
\[ ∑_{n=0}^{+∞} \PS{e_n}{x}^2 ≤ \Norm{x}^2. \]
De plus, si l'on suppose que $\Vect(\mathcal{F})$ est dense dans $E$,
on a alors l'égalité (dite de Parseval).

% -----------------------------------------------------------------------------
\subsection{Sous-espaces orthogonaux}

\Para{Définitions}
Soit $F$ et $G$ deux sous-espaces vectoriels et $x∈E$.
\begin{itemize}
\item \emph{$x$ est orthogonal à $F$}
  si et seulement si $x$ est orthogonal à tout vecteur de $F$.
  On note alors $x \orth F$.
\item \emph{$F$ est orthogonal à $G$}
  si et seulement si pour tout $(x,y)∈F×G$, on a $x \orth y$.
  On note alors $F \orth G$.
\end{itemize}

\Para{Définition}
Soit $A⊂E$.
On définit \emph{l'orthogonal de $A$}, noté $\Orth A$,
comme étant $\Ensemble{x∈E}{x \orth A}$.
Autrement dit,
\[ ∀x∈E\+ \Big( x∈\Orth A \iff ∀a∈A\+ \PS ax = 0 \Big). \]

\Para{Proposition}
Soit $A⊂E$.
Alors:
\begin{itemize}
\item $\Orth A$ est un sous-espace vectoriel de $E$;
\item $\Orth A = \Orth{\Vect(A)}$;
\item $x \orth A$ si et seulement si $x∈\Orth A$.
\end{itemize}

\Para{Proposition}
Soit $\Uplet{F_1}{F_n}$ des sous-espaces vectoriels de $E$ deux à deux orthogonaux.
Alors ils sont en somme directe.

\Para{Définition}
Soit $\Uplet{F_1}{F_n}$ des sous-espaces vectoriels de $E$ deux à deux orthogonaux.
On dit qu'ils sont en \emph{somme directe orthogonale} et l'on note leur somme
\[ ∑_{k=1}^n F_k = ⨁_{k=1}^{\substack{n\\⊥}} F_k. \]
De plus, si cette somme vaut $E$, on dit qu'ils sont \emph{supplémentaires orthogonaux}.

\Para{Théorème}
Soit $F$ un sous-espace vectoriel \emph{de dimension finie} de $E$.
Alors $E = F ⊕ F^{⊥}$ et $F^{⊥⊥} = F$.

\Para{Remarques}
\begin{enumerate}
\item On ne fait pas d'hypothèse sur la dimension de $E$.
\item Si l'on ne suppose plus que $F$ est de dimension finie, on a seulement $F⊕\Orth F⊂E$ et $F^{⊥⊥}⊃F$,
  et ces deux inclusions peuvent être strictes.
\end{enumerate}

% -----------------------------------------------------------------------------
\subsection{Projection orthogonale}

\Para{Notation}
Dans ce paragraphe, $F$ désignera toujours
un sous-espace vectoriel de $E$ tel que $E = F ⊕ \Orth F$;
c'est toujours le cas si $F$ est de dimension finie.

\Para{Définition}
On appelle \emph{projection orthogonale} sur $F$ la projection, notée $p_F$, sur $F$ parallèlement à $\Orth F$.

Autrement dit, si
\[ x = \underbrace{y}_{\vphantom{\Orth F}∈F} + \underbrace{\vphantom{y}z}_{∈\Orth F}, \]
alors $p_F(x) = y$.

\Para{Théorème}
Soit $(x,y)∈E^2$. £LCSSE.
\begin{enumerate}
\item $y = p_F(x)$.
\item $y∈F$ et $x - y∈\Orth F$.
\item $y∈F$ et $∀z∈F\+ \Norm{x - z}≥\Norm{x - y}$.
\end{enumerate}

\Para{Corollaires}
Soit $x∈E$.
\begin{enumerate}
\item Pour tout $y∈F$, on a $\Norm{x-y}≥\Norm{x-p_F(x)}$
  avec égalité si et seulement si $y = p_F(x)$.
\item La fonction $\Fonction{f}{F}{ℝ}{y}{d(x,y) = \Norm{x-y}}$
  atteint son minimum en un unique point $y = p_F(x)$.
\item $\inf_{y∈F} \Norm{x - y} = \Norm{x - p_F(x)}$.
\item $d(x,F) = d(x,p_F(x)) = \Norm{x - p_F(x)}$.
\end{enumerate}

\Para{Proposition}
On suppose que $F$ est de dimension finie.
Soit $\nUplet e1n$ une base orthonormale de $F$.
Alors, pour tout $x∈E$, la projection orthogonale de $x$ sur $F$ est donnée par la formule
\[ p_F(x) = ∑_{k=1}^n \PS{e_k}{x} e_k. \]

\Para{Remarque}[calcul pratique]
On suppose que $F$ est de dimension finie.
Soit $\B = \nUplet u1n$ une base de $F$ et $x∈E$.
Pour calculer $p_F(x)$, on a en général deux méthodes:
\begin{enumerate}
\item On orthonormalise $\B$
  puis on applique ensuite la formule de la proposition précédente.
  Le problème est que les calculs de l'orthonormalisation sont souvent peu agréables.
\item On note $p_F(x) =∑_{j=1}^nα_j u_j$
  et on écrit
  $∀i∈\Dcro{1,n}$, $\PS{u_i}{p_F(x)-x} = 0$.
  Cela fournit un système de $n$ équations linéaires à $n$ inconnues:
  \[ \rule{-1\labelwidth}{0pt}
  \left\{ \begin{array}{c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c}
    α_1 \PS{u_1}{u_1} & {}+{} & α_2 \PS{u_1}{u_2} & {}+{} & \cdots & {}+{} & α_n \PS{u_1}{u_n} & {}={} & \PS{u_1}{x} \\
    α_1 \PS{u_2}{u_1} & {}+{} & α_2 \PS{u_2}{u_2} & {}+{} & \cdots & {}+{} & α_n \PS{u_2}{u_n} & {}={} & \PS{u_2}{x} \\
    \cdots            &       & \cdots            &       & \cdots &       & \cdots            &       & \cdots      \\
    α_1 \PS{u_n}{u_1} & {}+{} & α_2 \PS{u_n}{u_2} & {}+{} & \cdots & {}+{} & α_n \PS{u_n}{u_n} & {}={} & \PS{u_n}{x}
  \end{array} \right. \]
  Ce système est nécessairement de Cramer.
  Il ne reste plus qu'à le résoudre, ce qui nous donne $\nUpletα1n$ et donc $p_F(x)$.
\end{enumerate}

\Para{Définition}
On appelle \emph{symétrie orthogonale} par rapport à $F$
la symétrie, notée $s_F$, par rapport à $F$ et parallèlement à $\Orth F$.
Autrement dit, si
$x = y + z$ où $y∈F$ et $z∈\Orth F$,
alors $s_F(x) = y - z$.

\Para{Proposition}
Avec les mêmes notations, $s_F = 2p_F - \Id_E$.

\Para{Définition}
Une \emph{réflexion} est une symétrie orthogonale par rapport à un hyperplan.

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
\section{Espaces euclidiens}

\Para{Définition}
Un \emph{espace euclidien} est un espace préhilbertien réel de dimension finie.

\Para{Notation}
À partir de maintenant, $E$ désignera toujours un espace euclidien de dimension $n≥1$.

\Para{Théorème}[base orthonormale incomplète]
On peut compléter toute famille orthonormale de vecteurs de $E$ en une base orthonormale de $E$.

\Para{Théorème}[Riesz]
Soit $\Fn fEℝ$ une forme linéaire sur $E$.
Alors il existe un unique $a∈E$ tel que pour tout $x∈E$, $f(x) = \PS ax$.

\Para{Corollaire}
Tout hyperplan de $E$ est de la forme $\Orth{\Acco x}$
pour un $x∈E∖\acco{0_E}$.

\Para{Lemme}
On note $(\,⋅\, | \,⋅\,)$ le produit scalaire usuel sur $ℝ^n$.
Soit $\B$ une £bon. de $E$.
Soit $x$ et $y$ deux vecteurs de $E$.
On pose $X = \Coords_\B(x)$ et $Y = \Coords_\B(y)$.
Alors $\PS{x}{y} = (X\mid Y)$.

% -----------------------------------------------------------------------------
\subsection{Endomorphismes symétriques}

\Para{Définition}
Soit $u∈\LE$.
On dit que $u$ est un endomorphisme \emph{symétrique} (ou \emph{autoadjoint}) si et seulement si $∀(x,y)∈E^2$, $\PS{u(x)}{y} = \PS{x}{u(y)}$.
On note $\SE$ l'ensemble des endomorphismes symétriques de $E$.
Il s'agit d'un sous-espace vectoriel de $\LE$.

\Para{Proposition}
Soit $u∈\LE$.
Les conditions suivantes sont équivalentes:
\begin{enumerate}[label=\roman*.]
\item $u$ est un endomorphisme symétrique;
\item pour toute base orthonormale $\B$ de $E$,
  $\Mat_\B(u)$ est une matrice symétrique;
\item il existe une base orthonormale $\B$ de $E$
  telle que $\Mat_\B(u)$ est une matrice symétrique.
\end{enumerate}

% -----------------------------------------------------------------------------
\subsection{Endomorphismes orthogonaux}

\Para{Proposition}
Soit $M∈\MnR$. Les conditions suivantes sont équivalentes:
\begin{enumerate}[label=\roman*.]
\item $\T M M = I_n$;
\item $M \T M = I_n$;
\item $M$ est inversible et $M^{-1} = \T M$;
\item les colonnes de $M$
  forment une base orthonormale de $ℝ^n$ muni du produit scalaire usuel;
\item les lignes de $M$
  forment une base orthonormale de $ℝ^n$ muni du produit scalaire usuel.
\end{enumerate}

\Para{Définition}
On dit que la matrice $M$ est \emph{orthogonale} lorsque ces conditions sont vérifiées.
On note $\OnR$ l'ensemble des matrices orthogonales de $\MnR$.

\Para{Proposition}
$\OnR$ est un sous-groupe de $(\GLnR,×)$; c'est pourquoi $\OnR$ est appelé le \emph{groupe orthogonal}.
Autrement dit:
\begin{enumerate}
\item Si $A∈\OnR$, alors $A$ est une matrice inversible et $A^{-1}∈\OnR$.
\item Si $(A,B)∈\OnR$, alors $AB∈\OnR$.
\end{enumerate}

\Para{Proposition}
La matrice de passage entre deux bases orthonormées de $E$ est une matrice orthogonale.

\Para{Définition}
Soit $u∈\LE$.
On dit que $u$ est un endomorphisme \emph{orthogonal} si et seulement si
$∀(x,y)∈E^2$, $\PS{u(x)}{u(y)} = \PS xy$.
On note $\OE$ l'ensemble des endomorphismes orthogonaux de $E$.
Il s'agit d'un sous-groupe de $\GLE$.

\Para{Définition}
Soit $u∈\LE$.
On dit que $u$ est une \emph{isométrie}
si et seulement si $∀x∈E$, $\Norm{u(x)} = \Norm{x}$.

\Para{Proposition}
Soit $u∈\LE$.
Alors $u$ est un endomorphisme orthogonal si et seulement si $u$ est une isométrie.

\Para{Proposition}
Soit $u∈\LE$. Les conditions suivantes sont équivalentes:
\begin{enumerate}[label=\roman*.]
\item $u$ est un endomorphisme orthogonal;
\item pour toute base orthonormale $\B$ de $E$,
  $\Mat_\B(u)$ est une matrice orthogonale;
\item il existe une base orthonormale $\B$ de $E$ telle que
  $\Mat_\B(u)$ est une matrice orthogonale.
\end{enumerate}

% -----------------------------------------------------------------------------
\subsection{Théorème spectral}

\Para{Lemmes techniques}
\begin{enumerate}
\item Soit $u∈\SE$.
  Alors les sous-espaces propres de $u$ sont deux à deux orthogonaux.
\item Soit $u∈\SE$ et $F$ un sous-espace vectoriel stable par $u$.
  Alors $\Orth F$ est stable par $u$.
\item Soit $A∈\SnR$.
  Alors $\Sp_ℂ(A)⊂ℝ$.
\item Soit $u∈\SE$.
  Alors $\Sp u≠∅$.
\end{enumerate}

\Para{Théorème}[théorème spectral]
Soit $u$ un endomorphisme symétrique de $E$.
Alors $u$ est \emph{diagonalisable en base orthonormale}, £cad. qu'il existe
une base orthonormale $\B$ telle que $\Mat_\B(u)$ soit diagonale.
De plus, les sous-espaces propres de $u$ sont supplémentaires orthogonaux, £cad.
\[ E = \mathop{⨁}^⊥_{λ∈\Sp u} E_λ \]
où $E_λ = \Ker(u-λ\Id_E)$.

\Para{Théorème}[théorème spectral, version matricielle]
Soit $A∈\SnR$.
Alors il existe $P∈\OnR$ et $D∈\DnR$
tels que $A = P D P^{-1} = P D \T P$.

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
\section{Un peu de géométrie}

\Para{Proposition-Définition}
Soit $(x,y)∈E^2$ deux vecteurs non nuls.
Alors il existe un unique $θ∈[0,π]$ tel que
\[ \PS{x}{y} = \Norm{x} \Norm{y} \cosθ, \]
et ce $θ$ s'appelle l'\emph{angle géométrique} entre les vecteurs $x$ et $y$.

\Para{Proposition}
Soit $f∈\OE$. Alors
\begin{itemize}
\item $\det f =±1$,
\item $\Sp_ℝf⊂\Acco{-1,1}.$
\end{itemize}

\Para{Définitions}
Soit $f∈\OE$.
\begin{itemize}
\item Si $\det f = 1$,
  on dit que $f$ est une \emph{isométrie directe}.
  L'ensemble des isométries directes de $E$ se note $\mathcal{O}^+(E)$ ou
  $\mathcal{SO}(E)$; il s'agit d'un sous-groupe de $\OE$.
\item Si $\det f = -1$,
  on dit que $u$ est une \emph{isométrie indirecte}.
  L'ensemble des isométries directes de $E$ se note $\mathcal{O}^-(E)$.
\end{itemize}

\Para{Proposition}
Soit $A∈\OnR$. On a nécessairement $\det A =±1$.

\Para{Définitions}
L'ensemble des matrices de $\OnR$ de déterminant~$1$
se note $\mathrm{O}_n^+(ℝ)$ ou $\mathrm{SO}_n(ℝ)$;
il s'agit d'un sous-groupe de $\OnR$ appelé
\emph{groupe spécial orthogonal}.
L'ensemble des matrices de $\OnR$ de déterminant~$-1$
se note $\mathrm{O}_n^-(ℝ)$.

% -----------------------------------------------------------------------------
\subsection{Déterminant, rappels et compléments}

\Para{Définition}
Soit $n$ la dimension de $E$, $\B$ une base de $E$ et $\nUplet x1n$ des vecteurs de $E$.
On note $\det_\B\nUplet x1n$ le déterminant de la matrice dont la $j$-ème colonne vaut
$\Coords_\B(x_j)$.

\Para{Proposition}
Soit $u∈\LE$ et $\B$ une base de $E$.
Soit $\nUplet x1n∈E^n$ où $n = \dim E$.
Alors \[ \det_\B \bigPa{\Uplet{u(x_1)}{u(x_n)}} = \det(u) \det_\B \nUplet x1n. \]

\Para{Proposition}
Soit $\B$, $\B'$ deux bases de $E$ et $P$ la matrice de passage de $\B$ à $\B'$.
Soit $\nUplet x1n∈E^n$ où $n = \dim E$.
Alors \[ \det_\B \nUplet x1n = \det(P) \det_{\B'} \nUplet x1n. \]

\Para{Définition}
Soit $\B$ et $\B'$ deux bases de $E$.
On dit que $\B$ et $\B'$ ont \emph{même orientation} £ssi. $\det\bigPa{\Pass(\B,\B')} > 0$.

\Para{Proposition}
Il s'agit d'une relation d'équivalence, £cad.:
\begin{enumerate}
\item \emph{réflexivité}: toute base a la même orientation qu'elle-même;
\item \emph{symétrie}: si $\B$ et $\B'$ ont la même orientation, alors $\B'$ et $\B$ ont la même orientation;
\item \emph{transitivité}: deux bases ayant la même orientation qu'une même troisième ont la même orientation.
\end{enumerate}
Pour cette relation, il y a deux classes d'équivalences.

\Para{Définition}
Orienter un espace vectoriel, c'est convenir d'appeler \emph{directes} les bases de l'une des deux classes, et \emph{indirectes} les bases de l'autre.

% -----------------------------------------------------------------------------
\subsection{Classification des isométries du plan}

\Para{Contexte}
On se place dans le cadre d'un espace euclidien orienté $E$ de dimension $2$.

\Para{Lemme}
Soit $M∈\mathrm{O}_2^+(ℝ)$.
Alors il existe $θ∈ℝ$, unique modulo $2π$, tel que
\[ M = R_θ = \Matrix{\cosθ,-\sinθ;\sinθ,\cosθ}. \]
De plus, $R_θR_{θ'} = R_{θ+θ'}$.

\Para{Théorème}
Soit $f∈\mathcal{O}^+(E)$.
Alors il existe un $θ∈ℝ$, unique modulo $2π$, tel que
pour toute base orthonormale directe $\B$ de $E$, on ait $\Mat_\B(f) = R_θ$.
On dit que $f$ est la \emph{rotation d'angle $θ$}.
De plus, si $x∈E$ vérifie $\Norm x = 1$,
alors $\cosθ= \PS{x}{f(x)}$ et $\sinθ= \det(x,f(x))$.

\Para{Théorème}
Soit $f∈\mathcal{O}^-(E)$.
Alors pour toute base orthonormale directe $\B$ de $E$,
il existe $θ∈ℝ$, unique modulo $2π$, tel que
\[ \Mat_\B(f) = \Matrix{\cosθ,\sinθ;\sinθ,-\cosθ}. \]
De plus, $f$ est la réflexion d'axe le vecteur de coordonnées $\Matrix{\cos(θ/2);\sin(θ/2)}$.

% -----------------------------------------------------------------------------
\subsection{Produit mixte, produit vectoriel}

\Para{Contexte}
On se place dans le cadre d'un espace euclidien orienté $E$
de dimension $3$.

\Para{Proposition}
Soit $\B_1$ et $\B_2$ deux bases orthonormées directes de $E$.
Alors $\det_{\B_1} = \det_{\B_2}$,
£cad.
\[ ∀(x,y,z)∈E^3\+ \det_{\B_1}(x,y,z) = \det_{\B_2}(x,y,z). \]

\Para{Définition}
Soit $(x,y,z)∈E^3$.
On définit le \emph{produit mixte} des trois vecteurs $x$, $y$ et $z$ par
\[ \Pmixte xyz = \det_\B(x,y,z) \]
pour une base orthonormée directe $\B$ quelconque.

\Para{Proposition-Définition}
Soit $(u,v)∈E^2$.
Alors il existe un unique vecteur $w∈E$ tel que
\[ ∀x∈E, \quad \Pmixte uvx = \PS{w}{x}. \]
Le vecteur $w$, que l'on note $u \wedge v$, s'appelle le \emph{produit vectoriel} de $u$ et $v$.

\Para{Remarque}
Le produit vectoriel est donc entièrement caractérisé par la formule
\[ ∀(x,y,z)∈E^3\+ \Pmixte xyz = \PS{x \wedge y}{z}. \]

\Para{Proposition}
Si $(u,v,w)∈E^2$ et $(λ,μ)∈ℝ^2$, on a:
\begin{itemize}
\item $(λu+μv) \wedge w = λ(u \wedge w) + μ(v \wedge w)$;
\item $u \wedge (λv+μw) = λ(u \wedge v) + μ(u \wedge w)$;
\item $u \wedge u = 0$;
\item $u \wedge v = - (v \wedge u)$;
\item $u \wedge v∈\Orth{\Vect(u,v)}$;
\item $u \wedge v = 0$ si et seulement si $u$ et $v$ sont liés;
\item si $(u,v)$ est libre,
  alors $(u,v,u \wedge v)$ est une base directe de $E$;
\item si $(u,v)$ est orthonormée,
  alors $(u,v,u \wedge v)$ est une base orthonormée directe de $E$;
\item $u \wedge (v \wedge w) = \PS{u}{w}{v} - \PS{u}{v}{w}$.
  Il s'agit de la \emph{formule du double produit vectoriel};
\item si $θ$ est l'angle $(u,v)$,
  alors
  $\PS uv = \Norm u \Norm v \cosθ$
  et
  $\Norm{u \wedge v} = \Norm u \Norm v \, \Abs{\sinθ}$.
\end{itemize}

% -----------------------------------------------------------------------------
\subsection{Classification des isométries de l'espace}

\Para{Contexte}
On se place dans le cadre d'un espace euclidien orienté $E$ de dimension $3$.

\Para{Proposition-Définition}
Soit $f∈\mathcal{O}^+(E)$.
Alors il existe une base orthonormée directe $\B = (u_1,u_2,u_3)$
telle que
\[ \Mat_\B(f) = \Matrix{1,0,0;0,\cosθ,-\sinθ;0,\sinθ,\cosθ} = \Matrix{1,;,R_θ}. \]
On dit que $f$ est la rotation d'axe $u_1$ et d'angle $θ$.
De plus,
\begin{itemize}
\item $\Tr f = 1+2\cosθ$,
  ce qui permet de déterminer $θ$ au signe près;
\item si la famille $(u_1,v)$ est libre,
  alors $\Signe(\sinθ) = \Signe \Pmixte{u_1}{v}{f(v)}$.
\end{itemize}

\Para{Proposition}
Soit $f∈\mathcal{O}^-(E)$.
Alors il existe une base orthonormée directe $\B = (u_1,u_2,u_3)$
telle que
\[ \Mat_\B(f) = \Matrix{-1,0,0;0,\cosθ,-\sinθ;0,\sinθ,\cosθ} = \Matrix{-1,;,R_θ}. \]
$f$ est la composée commutative de la rotation d'axe $u_1$ et d'angle $θ$
et de la réflexion par rapport au plan $\Orth{\Vect(u_1)}$.
De plus,
\begin{itemize}
\item $\Tr f = -1+2\cosθ$,
  ce qui permet de déterminer $θ$ au signe près;
\item si la famille $(u_1,v)$ est libre,
  alors $\Signe(\sinθ) = \Signe \Pmixte{u_1}{v}{f(v)}$.
\end{itemize}

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
\section{Exercices}

% -----------------------------------------------------------------------------
\subsection{Espaces préhilbertiens}

\subsubsection{Produits scalaires}

\Exercice \label{exo:ps1}

Vérifier que $\PS⋅⋅$ est un produit scalaire sur $E$ dans les cas suivants.
\begin{enumerate}
\item $E = \MnR$
  et $\PS AB = \Tr\bigPa{\T{A}B}$.
\item $E = \mathcal{C}([-1,1],ℝ)$
  et $\DS \PS fg = ∫_{-1}^1 \frac{f(t)g(t)}{√{1-t^2}}\D t$.
\item $E = \BigEnsemble{\Fn{f}{ℝ}{ℝ}\text{ continues}}{ f^2 \text{ intégrable sur }ℝ}$
  et $\DS \PS fg = ∫_ℝ fg$.
\item $E = ℝ[X]$
  et $\DS \PS PQ =∫_0^1 P(t)Q(t)\D t$.
\item $E = ℝ[X]$
  et $\DS \PS PQ =∑_{n=0}^{+∞} \frac{P(n)Q(n)}{2^n}$.
\end{enumerate}

\Exercice \label{exo:ps2}
Soit $n∈ℕ$ fixé, $E = ℝ_n[X]$ et $F = \Ensemble{P∈E}{P(0)=P(1)=0}$.
Pour $(P,Q)∈E$, on pose \[ φ(P,Q) = -∫_0^1 (PQ''+P''Q). \]
\begin{enumerate}
\item Vérifier que $F$ est un espace vectoriel.
\item Donner une base et la dimension de $F$.
\item $φ$ définit-il un produit scalaire sur $E$? sur $F$?
\end{enumerate}

\Exercice \label{exo:ps3}

Soit $E$ euclidien et $\B = \nUplet e1n$ une bon de $E$.
On pose \[ \Fonction{φ}{\LE×\LE}{ℝ}{(u,v)}{∑_{i=1}^n \PS{u(e_i)}{v(e_i)}} \]
Montrer que $φ$ est un produit scalaire sur $\LE$,
et déterminer une base orthonormale pour ce produit scalaire.

\Exercice \label{exo:ps4}

Soit $E$ l'ensemble des suites réelles $(u_n)_{n∈ℕ}$ telles que la série de terme général $u_n^2$ converge.
Pour $u$ et $v$ dans $E$, on pose
\[ \PS uv = ∑_{n=0}^{+∞} u_n v_n. \]
\begin{enumerate}
\item Montrer que $E$ est un £ev.. On le note usuellement $ℓ^2$.
\item Montrer que $\PS uv$ existe.
\item Montrer qu'il s'agit d'un produit scalaire.
\end{enumerate}

\subsubsection{Polynômes orthogonaux}

\Exercice

On munit le $ℝ$-espace vectoriel $E = \mathcal{C}([-1,1],ℝ)$
du produit scalaire usuel défini par
\[ ∀(f,g)∈E^2 \+ \PS fg = ∫_{-1}^1 f(t) g(t) \D t. \]
On pose pour tout $n∈ℕ$,
\[ L_n(X) = \frac{1}{2^n n!}⋅\frac{\mathrm{d}^n}{\mathrm{d}X^n} \Big[ (X^2-1)^n \Big] \]
Les polynômes $\bigl(L_n(X)\bigr)_{n∈ℕ}$ s'appellent \emph{polynômes de Legendre}.
On pourra introduire $H_n(X) = (X^2-1)^n$.\begin{enumerate}
\item Montrer que $L_n$ est un polynôme de degré $n$
  dont on précisera le coefficient dominant.
\item En utilisant la formule de Leibniz, calculer $L_n(1)$ et $L_n(-1)$.
\item Avec une intégration par parties multiple, calculer $\PS{L_n}{L_n}$.
\item Calculer $\PS{Q}{L_n}$ lorsque $Q$ est un polynôme de $ℝ_{n-1}[X]$.
\item En déduire $\PS{L_n}{L_m}$ lorsque $n≠m$.
\item Comparer $(L_n)_{n∈ℕ}$ à l'orthonormalisée de la base canonique.
\end{enumerate}

\Exercice \label{exo:ps5}

Soit $E = ℝ[X]$.
On pose, pour $(P,Q)∈E^2$,
\[ \PS PQ = \frac2π∫_{-1}^1√{1-t^2} \, P(t) Q(t) \D t \]\begin{enumerate}
\item Montrer que $\PS⋅⋅$ est bien un produit scalaire sur $E$.
\item Soit $n∈ℕ$. Montrer qu'il existe un unique polynôme $U_n$
  tel que
  \[ ∀θ∈ℝ\+
  \sin\bigl( (n+1)θ\bigr) = \sin(θ) U_n(\cosθ). \]

  Les polynômes $\bigl(U_n(X)\bigr)_{n∈ℕ}$ s'appellent
  \emph{polynômes de Tchebychev de seconde espèce}.
\item Comparer $(U_n)_{n∈ℕ}$ à l'orthonormalisée de la base canonique.
\end{enumerate}

\subsubsection{Projection orthogonale}

\Exercice

Soit $E$ un espace euclidien et $\B = (e_1,e_2,e_3,e_4)$ une £bon. de $E$.
On considère le sous-espace vectoriel $F$ d'équations dans $\B$:
\[ \left\{ \begin{alignedat}{5}
  x_1 & {}+{} & x_2  & {}+{} & x_3  & {}+{} & x_4  & {}={} & 0 \\
  x_1 & {}+{} & 2x_2 & {}+{} & 3x_3 & {}+{} & 4x_4 & {}={} & 0
\end{alignedat} \right. \]
\begin{enumerate}
\item Quelle est la dimension de $F$?
\item Déterminer $\Orth F$.
\item Trouver une £bon. de $F$.
\item Donner la matrice de la projection orthogonale sur $F$.
\item Calculer la distance de $e_1=(1,0,0,0)$ à $F$.
\end{enumerate}

\Exercice

Soit $\mathcal{E} = \MnR$ muni du produit scalaire usuel
$\PS AB = \Tr\bigl(\T A B\bigr)$.\begin{enumerate}
\item Montrer que $\mathcal{S}$ et $\mathcal{A}$ sont supplémentaires orthogonaux,
  où $\mathcal{S}$ désigne l'ensemble des matrices symétriques
  et $\mathcal{A}$ l'ensemble des matrices antisymétriques.
\item Montrer que
  \[ ∀A∈\mathcal{E} \+ \Tr A ≤ √{n\Tr(\T A A)}. \]
  Étudier le cas d'égalité.
\item Pour $A∈\mathcal{E}$, on pose
  \[ \Fonction{f_A}{\mathcal{S}}{ℝ}{S}{∑_{i=1}^n∑_{j=1}^n (a_{i,j} - s_{i,j})^2} \]
  Déterminer le minimum de $f_A$ et la matrice qui réalise ce minimum.
\end{enumerate}

\Exercice

Déterminer $(a,b,c,d)∈ℝ^4$ tels que l'intégrale
\[ ∫_{-π/2}^{π/2} \Big( \sin x - ax^3 - bx^2 - cx - d \Big)^2 \D x \]
soit minimale.

% -----------------------------------------------------------------------------
\subsection{Espaces euclidiens}

\subsubsection{Endomorphismes symétriques}

\Exercice

Soit $A$ une matrice symétrique réelle telle que $A^2 = 0$.
Montrer que $A = 0$.

\Exercice

Soit $A = (a_{ij})$ une matrice réelle symétrique.
On note $\Uplet{λ_1}{λ_n}$ ses valeurs propres.
Montrer que
\[ ∑_{k=1}^nλ_k^2 = ∑_{i=1}^n ∑_{j=1}^n a_{ij}^2. \]

\Exercice  \label{exo:ps6}

Soit $E = ℝ_n[X]$. Pour $(P,Q)∈E^2$, on pose
\[ \PS PQ = ∫_{-1}^1 \frac{P(t)Q(t)}{√{1-t^2}} \D t. \]
\begin{enumerate}
\item Montrer qu'il s'agit bien d'un produit scalaire.
\item Montrer que $u \colon P \mapsto (X^2-1)P''+XP'$ est un endomorphisme symétrique
  pour ce produit scalaire.
\end{enumerate}

\Exercice

Soit $A∈\SnR$ et $p∈ℕ^*$ telle que $A^p = I_n$.
Montrer que $A^2 = I_n$.

\Exercice

Soit $u∈\mathcal{S}(E)$. Montrer que $\Ker u$ et $\Ima u$ sont des
supplémentaires orthogonaux.

\Exercice

Soit $u, v∈\mathcal{S}(E)$.
Montrer que $u◦v$ est symétrique £ssi. $u$ et $v$ commutent.

\Exercice

Diagonaliser dans une base orthonormale
\[ A = \Matrix{6,-2,2;-2,5,0;2,0,7} \text{ et }
B = \frac19 \Matrix{23,2,-4;2,29,2;-4,2,23}. \]

\Exercice

Diagonaliser les matrices
\[ \Matrix{1,1,\cdots,1,1;1,0,\cdots,0,1;\vdots,\vdots,(0),\vdots,\vdots;1,0,\cdots,0,1;1,1,\cdots,1,1} \text{ et }
\Matrix{2,1,0,\cdots,\cdots,0;1,2,1,\ddots,(0),\vdots;0,1,2,1,\ddots,\vdots;\vdots,\ddots,\ddots,\ddots,\ddots,0;\vdots,(0),\ddots,1,2,1;0,\cdots,\cdots,0,1,2}. \]

\Exercice \label{exo:proj_orth_sym}

Soit $p$ un projecteur de $E$ de noyau $F$ et d'image $G$.
Montrer que les conditions suivantes sont équivalentes:
\begin{enumerate}[label=\roman*.]
\item Pour tout $x∈E$, $p(x)$ est la projection orthogonale de $x$ sur $G$.
\item $F$ et $G$ sont orthogonaux.
\item $p$ est un endomorphisme symétrique.
\end{enumerate}
On dit alors que $p$ est un \emph{projecteur orthogonal}.

\Exercice \label{exo:sym_orth_sym}

Soit $s$ une symétrie de $E$ d'axe $F$ parallèlement à $G$.
Montrer que les conditions suivantes sont équivalentes:
\begin{enumerate}[label=\roman*.]
\item $s$ est un endomorphisme symétrique.
\item $F$ et $G$ sont orthogonaux.
\end{enumerate}
On dit alors que $s$ est une \emph{symétrie orthogonale}.
Notez en particulier qu'une symétrie n'est pas nécessairement un endomorphisme symétrique.

\Exercice

Soit $M∈\MnR$.
\begin{enumerate}
\item Montrer que $M$ est la matrice d'un projecteur orthogonal dans une base orthonormale
  si et seulement si $M^2 = M$ et $M = \T M$.
\item Montrer que $M$ est la matrice d'une symétrie orthogonale dans une base orthonormale
  si et seulement si $M^2 = I_n$ et $M = \T M$.
\end{enumerate}
On pourra s'aider des exercices~\ref{exo:proj_orth_sym} et~\ref{exo:sym_orth_sym}.

\subsubsection{Endomorphismes symétriques positifs}

\Exercice

Soit $u∈\SE$.
\begin{enumerate}
\item Montrer que $\Sp(u)⊂ℝ^+$ £ssi. $∀x∈E$, $\PS{u(x)}{x}≥0$.
  Un tel endormorphisme est dit \emph{symétrique positif}.
  On note $\SpE$ leur ensemble.
\item Montrer que $\Sp(u)⊂ℝ^+_*$ £ssi. $∀x∈E∖\acco{0_E}$, $\PS{u(x)}{x} > 0$.
  Un tel endormorphisme est dit \emph{symétrique défini positif}.
  On note $\SppE$ leur ensemble.
\item Montrer que $\SppE = \SpE∩\GLE$.
\end{enumerate}

\Exercice

Soit $A∈\SnR$.\begin{enumerate}
\item Montrer que les conditions suivantes sont équivalentes:
  \begin{enumerate}[label=\roman*.]
  \item $∀X∈\mathrm{M}_{n,1}(ℝ)$, $\T{X} A X≥0$
  \item $\Sp A⊂ℝ^+$
  \end{enumerate}
  Une matrice symétrique vérifiant ces conditions est dite positive.
  L'ensemble des matrices symétriques positives se note $\SnRp$.
\item Montrer que les conditions suivantes sont équivalentes:
  \begin{enumerate}[label=\roman*.]
  \item $∀X∈\mathrm{M}_{n,1}(ℝ)∖\acco0$, $\T{X} A X > 0$
  \item $\Sp A⊂ℝ^+_*$
  \item $A∈\SnRp∩\GLnR$
  \end{enumerate}
  Une matrice symétrique vérifiant ces conditions est dite définie positive.
  L'ensemble des matrices définies symétriques positives se note $\SnRpp$.
\end{enumerate}

\Exercice

La matrice $A = \Matrix{4,1,1;1,4,1;1,1,4}$
est-elle une matrice symétrique définie positive?

\Exercice

Soit $A∈\MnR$. Montrer que $A$ est symétrique définie positive
si et seulement si il existe $M$ inversible telle que $A = \T{M} M$.

\Exercice

Soit $A∈\SnRp$.
Montrer qu'il existe une unique matrice $B∈\SnRp$ telle que $A = B^2$.
Pour l'unicité, on pourra se ramener à des endomorphismes.

Calculer $B$ lorsque $A = \Matrix{1,2;2,5}$.

\Exercice[diagonalisation simultanée]

Soit $A∈\SnRpp$ et $B∈\SnR$.
Le but de cet exercice est de démontrer le théorème de réduction simultanée,
qui affirme qu'il existe $(M,D)∈\GLnR×\DnR$ telles que
$A = \T{M} M$ et $B = \T{M} D M$.\begin{enumerate}
\item Montrer qu'il existe $P∈\OnR$ et $E∈\DnR$ telles que $A = P E P^{-1}$.
\item Montrer qu'il existe $F∈\DnR$ telle que $E = F^2$.
\item On pose $B' = F^{-1} P^{-1} B P F^{-1}$.
  Montrer que $B'∈\SnR$, et en déduire qu'il existe $Q∈\OnR$
  et $D∈\DnR$ telles que $B' = QDQ^{-1}$.
\item On pose $M = Q^{-1} F P^{-1}$. Calculer $\T{M} M$ et $\T{M} D M$. Conclure.
\end{enumerate}

\Exercice

Soit $A∈\SnRpp$ et $B∈\SnRp$.
Montrer que $\det(A+B)≥\det A + \det B$.

On pourra utiliser le résultat de l'exercice précédent.

\subsubsection{Endomorphismes orthogonaux}

\Exercice

Les projecteurs orthogonaux sont-ils des endomorphismes orthogonaux?
Même question pour les symétries orthogonales.

\Exercice

Quels sont les endomorphismes orthogonaux diagonalisables?

\Exercice

Soit $A∈\OnR$.
Montrer que $∀λ∈\Sp_ℂA$, on a $\Abs{λ} = 1$.

\Exercice

Quels sont les endomorphismes à la fois symétriques et orthogonaux?

\Exercice

Soit $A = (a_{i,j})∈\OnR$.
Montrer que \[ \left|∑_{i=1}^n∑_{j=1}^n a_{i,j} \right|≤n. \]
Cas d'égalité?

\subsubsection{Adjoint}

\Exercice[facile, mais important]

Soit $E$ euclidien, $(x,y)∈E^2$, $\B$ une base de $E$,
$X = \Coords_\B(x)$ et
$Y = \Coords_\B(y)$.\begin{enumerate}
\item Si $\B$ est une base orthonormale, montrer que $\PS xy = \T XY$.
\item Dans le cas général,
  montrer qu'il existe une matrice $A∈\SnR$ telle que $\PS xy = \T XAY$.
  Expliciter $A$.
\end{enumerate}

\Exercice

Soit $E$ euclidien, $u∈\LE$.\begin{enumerate}
\item Soit $u∈\LE$, $\B$ une base orthonormale de $E$ et $A = \Mat_\B(u)$.
  On définit $v∈\LE$ par la relation $\Mat_\B(v) = \T A$.
  Montrer que
  \[ ∀(x,y)∈E^2 \+ \PS{u(x)}{y} = \PS{x}{v(y)}. \]
\item En déduire que pour tout $u∈\LE$,
  il existe un unique $v∈\LE$ tel que
  \[ ∀(x,y)∈E^2, \quad \PS{u(x)}{y} = \PS{x}{v(y)} \]
  $v$ s'appelle l'\emph{adjoint} de $u$ et se note $v=u^*$.
\item Vérifier que dans toute base orthonormale $\B'$, on a
  \[ \Mat_{\B'}(u^*) = \T{\Mat_{\B'}(u)}. \]
\end{enumerate}

\Exercice

Soit $E$ euclidien et $u∈\LE$.\begin{enumerate}
\item Montrer que $u∈\SE$ si et seulement si $u^* = u$.
\item Montrer que $u∈\OE$ si et seulement si $u$ est inversible d'inverse $u^*$.
\end{enumerate}

\subsubsection{Divers}

\Exercice

Soit $E$ un espace euclidien de dimension $n$,
$\B$ une base orthonormale de $E$ et
$\nUplet x1n$ une famille de vecteurs de $E$.
Le but de cet exercice est de montrer l'inégalité suivante:
\[ \Abs{ \det_\B \nUplet x1n }
≤\Norm{x_1}⋅\Norm{x_2}⋅\cdots⋅\Norm{x_n} \]\begin{enumerate}
\item Montrer que le résultat est vrai si $\nUplet x1n$ n'est pas libre.
\item On suppose désormais $\nUplet x1n$ libre.
  On utilise le procédé de Schmidt,
  et on obtient $\nUplet y1n$ orthogonale et $\nUplet z1n$ orthonormale.
  Montrer par récurrence sur $k$ que
  \[ \det_\B \nUplet x1n
  = \det_\B (y_1, \cdots, y_k, x_{k+1}, \cdots x_n) \]
\item Conclure.
\end{enumerate}

% -----------------------------------------------------------------------------
\subsection{Un peu de géométrie}

\Exercice

Compléter la matrice $A = \frac1\star \Matrix{6,3,\star;-2,6,\star;3,\star,\star}$
en une matrice orthogonale positive,
et décrire la transformation géométrique associée à $A$.

\Exercice

Reconnaître les endomorphismes de $ℝ^3$ muni de sa structure euclidienne
usuelle canoniquement associés aux matrices suivantes:
\[ \frac13\Matrix{-2,-1,2;2,-2,1;1,2,2},
\frac19\Matrix{-7,-4,4;4,-8,-1;-4,-1,-8},
\Matrix{8,1,4;-4,4,7;1,8,-4}. \]

\Exercice

Déterminer la matrice de la rotation $r$ de $ℝ^3$ dans une base orthonormée
$(i, j, k)$ telle que $r(u) = u$
avec $u = i - j + k$ et $r(i) = k$.
Donner son angle de rotation.

\Exercice

On considère l'endomorphisme $f$ de $ℝ^3$ de matrice dans la base canonique
\[ A = \Matrix{a^2,ab-c,ac+b;ab+c,b^2,bc-a;ac-b,bc+a,c^2} \]
où $(a,b,c)∈ℝ^3$.
Déterminer $a,b,c$ de sorte que $f$ soit une isométrie, et la préciser.

\Exercice

Reconnaître les endomorphismes de $ℝ^3$ définis par
\begin{enumerate}
\item $\left\{ \begin{alignedat}{4}
  3x' & {}={} & 2x  & {}+{} & 2y & {}+{} & z  \\
  3y' & {}={} & -2x & {}+{} & y  & {}+{} & 2z \\
  3z' & {}={} & x   & {}-{} & 2y & {}+{} & 2z
\end{alignedat} \right.$
\item $\left\{ \begin{alignedat}{4}
  4x' & {}={} & -2x      & {}-{} & y√6 & {}+{} & z√6 \\
  4y' & {}={} & x√6  & {}+{} & y       & {}+{} & 3z      \\
  4z' & {}={} & -x√6 & {}+{} & 3y      & {}+{} & z
\end{alignedat} \right.$
\item $\left\{ \begin{alignedat}{4}
  3x' & {}={} & x  & {}+{} & 2y & {}+{} & 2z \\
  3y' & {}={} & 2x & {}+{} & y  & {}-{} & 2z \\
  3z' & {}={} & 2x & {}-{} & 2y & {}+{} & z
\end{alignedat} \right.$
\item $\left\{ \begin{alignedat}{4}
  4x' & {}={} & -x      & {}+{} & 3y      & {}-{} & z√6       \\
  4y' & {}={} & 3x      & {}-{} & y       & {}-{} & z√6        \\
  4z' & {}={} & x√6 & {}+{} & y√6 & {}+{} & 2z
\end{alignedat} \right.$
\end{enumerate}

\Exercice

Soit $\DS A = \Matrix{a,b,c;c,a,b;b,c,a}$.
Montrer que A est une matrice orthogonale positive
si et seulement si $a$, $b$ et $c$ son les racines du polynôme
$P(X) = X^3 - X^2 + k$, où $0≤k≤\frac{4}{27}$.
En posant alors $k = \frac{4}{27} \sin^2θ$, déterminer l'axe
et l'angle de la rotation associée à $A$.

\end{document}
